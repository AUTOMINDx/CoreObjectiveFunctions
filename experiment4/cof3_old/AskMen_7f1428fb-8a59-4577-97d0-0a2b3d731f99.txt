I remember reading an article a while back about how, even though body image issues have always been a problem for women, it's recently become an issue for men as well. I think it's important to have a discussion about this, because it seems like society is still dismissing or downplaying the experiences of men who struggle with these issues. It's true that body image issues have always been a problem for women, but I think it's important to remember that this is also an issue for men. I find it frustrating that society is still dismissing or downplaying the experiences of men who struggle with these issues.