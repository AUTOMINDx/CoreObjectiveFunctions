Why did liberalism become the dominant ideology of the US left?
How did liberalism become the left-wing ideology in the US?  
I've done a bit of looking and it seems that liberalism is mostly the ideology of centrist minor parties in many advanced nations with a few examples of it being the ideology of the right, like in Australia and a few where it's also on the left like Canada. By far the most dominant leftist ideology in advanced nations is social democracy, yet this is only a minor faction in the US democratic party.

How did liberalism become the norm instead of social democracy?

How would the US differ if its main leftist ideology was social democracy instead of liberalism?